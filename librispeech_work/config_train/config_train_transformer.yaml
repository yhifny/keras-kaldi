seed: 2234
out_path: /media/lumi/alpha/asr_work_timit


model:
    input_feat: fmllr
    input_dim: 40
    encoders:
        encoder_1:
            layers:
                - type: dense
                  units: 512
                  activation: linear
                  
                - type: postional_embedding            

                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 8
                  d_hid: 1024
                  attention_dropout: 0.2
                  residual_dropout: 0.2                  
                  
                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 8
                  d_hid: 1024
                  attention_dropout: 0.2
                  residual_dropout: 0.2

                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 8
                  d_hid: 1024
                  attention_dropout: 0.2
                  residual_dropout: 0.2                  
                  
                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 8
                  d_hid: 1024
                  attention_dropout: 0.2
                  residual_dropout: 0.2                  

                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 8
                  d_hid: 1024
                  attention_dropout: 0.2
                  residual_dropout: 0.2                  

                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 8
                  d_hid: 1024
                  attention_dropout: 0.2
                  residual_dropout: 0.2                  
                  
                  
    outputs:
        output_tri:
            layers:        
                - type: dense
                  units: 3480
                  activation: linear
                - type: batchnormalization                  
                - type: activation
                  name: output_tri
                  function: softmax              

        output_mono:
            layers:        
                - type: dense
                  units: 347  # start from 1 
                  activation: linear
                - type: batchnormalization                  
                - type: activation
                  name: output_mono   
                  function: softmax
                  
    seq2seq-encoder-decoder1:
        encoder-decoder_1:
            encoder:
                - lstm_units: 500
            decoder:
                - lstm_units: 500
                  units: 349 # mono + <s> + </s>   
          
optimizer:          
    name: adam
    ngpu: 2
    lr: 0.0004 # 1E-4
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 0.00000001
    batch_size: 32
    
optimizer:          
    name: rmsprop
    ngpu: 2
    lr: 0.0004 # 1E-4
    rho: 0.95
    epsilon: 0.00000001
    batch_size: 8      