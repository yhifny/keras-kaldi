seed: 2234
out_path: /lfs01/workdirs/hlwn026u2/keras-kaldi/timit_work//asr_work_timit_contextnet
mean_std_file: mean_std_fmllr.npz


model:
    name: contextnet
    input_feat: fmllr
    input_dim: 40
    encoders:
     encoder:
      alpha: 0.5
      blocks:
        # C0
        - nlayers: 1
          kernel_size: 5
          filters: 256
          strides: 1
          residual: False
          activation: silu
        # C1-C2
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        # C3
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        # C4-C6
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        # C7
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        # C8 - C10
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 256
          strides: 1
          residual: True
          activation: silu
        # C11 - C13
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        # C14
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1

          residual: True
          activation: silu
        # C15 - C21
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        - nlayers: 5
          kernel_size: 5
          filters: 512
          strides: 1
          residual: True
          activation: silu
        # C22
        - nlayers: 1
          kernel_size: 5
          filters: 640
          strides: 1
          residual: False
          activation: silu

    outputs:
      output_tri:
        layers:
          - type: dense
            units: 1960
            activation: linear
          - type: batchnormalization
          - type: activation
            name: output_tri
            function: softmax

      output_mono:
        layers:
          - type: dense
            units: 49  # start from 1
            activation: linear
          - type: batchnormalization
          - type: activation
            name: output_mono
            function: softmax

    seq2seq-encoder-decoder1:
      encoder-decoder_1:
        encoder:
          - lstm_units: 500
        decoder:
          - lstm_units: 500
            context_units: 500
            units: 51 # mono + <s> + </s>


loss_weights: {'output_tri' :1.0, 'output_mono': 1.0}                  

optimizer:
    name: adam
    ngpu: 2
    lr: 0.0004 # 1E-4
    beta_1: 0.9
    beta_2: 0.98
    epsilon: 0.000000001
    batch_size: 2
    warmup_steps: 10000
          
