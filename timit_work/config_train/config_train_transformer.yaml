seed: 2234
out_path: /media/lumi/alpha/asr_work_timit


model:
    input_feat: fmllr
    input_dim: 40
    encoders:
        encoder_1:
            layers:
                - type: dense
                  units: 256
                  activation: linear
                  
                - type: postional_embedding            

                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 4
                  d_hid: 2048
                  attention_dropout: 0.01
                  residual_dropout: 0.01
                  name: t1  
                  
                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 4
                  d_hid: 2048
                  attention_dropout: 0.01
                  residual_dropout: 0.01
                  name: t2
                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 4
                  d_hid: 2048
                  attention_dropout: 0.01
                  residual_dropout: 0.01                  
                  name: t3                  
                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 4
                  d_hid: 2048
                  attention_dropout: 0.01
                  residual_dropout: 0.01                  
                  name: t4
                  
                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 4
                  d_hid: 2048
                  attention_dropout: 0.01
                  residual_dropout: 0.01                  
                  name: t5                 

                - type: transformer_encoder_layer
                  n_state: 64
                  n_head: 4
                  d_hid: 2048
                  attention_dropout: 0.01
                  residual_dropout: 0.01                  
                  name: t6                   


                  
    outputs:
        output_tri:
            layers:        
                - type: dense
                  units: 1928
                  activation: linear
                - type: batchnormalization                  
                - type: activation
                  name: output_tri
                  function: softmax              

        output_mono:
            layers:        
                - type: dense
                  units: 49  # start from 1 
                  activation: linear
                - type: batchnormalization                  
                - type: activation
                  name: output_mono   
                  function: softmax
                  
    seq2seq-encoder-decoder1:
        encoder-decoder_1:
            encoder:
                - lstm_units: 500
            decoder:
                - lstm_units: 500
                  units: 51 # mono + <s> + </s>
                  
loss_weights: {'output_tri' :1.0, 'output_mono': 1.0}          

optimizer1:          
    name: adam
    ngpu: 2
    lr: 0.001 # 1E-4
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 0.00000001
    batch_size: 8
    
optimizer:          
    name: rmsprop
    ngpu: 2
    lr: 0.001 # 1E-4
    rho: 0.95
    epsilon: 0.00000001
    batch_size: 8      